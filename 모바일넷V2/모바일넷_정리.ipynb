{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMHZxqjifvcL"
      },
      "outputs": [],
      "source": [
        "# 이미지 전처리\n",
        "\n",
        "bbox = []\n",
        "for label in labels:\n",
        "    width = label[\"W\"]\n",
        "    height = label[\"H\"]\n",
        "    point_str = label[\"Point(x,y)\"]\n",
        "\n",
        "    x, y = point_str.split(\",\")\n",
        "    x = float(x)\n",
        "    y = float(y)\n",
        "    width = float(width)\n",
        "    height = float(height)\n",
        "\n",
        "    # x_min = int(x * owidth)\n",
        "    # y_min = int(y * oheight)\n",
        "    # x_max = int((x + owidth) * owidth)\n",
        "    # y_max = int((y + oheight) * owidth)\n",
        "\n",
        "bbox.append((x, y, width, height))\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(width)\n",
        "print(height)\n",
        "\n",
        "for i, bbox_info in enumerate(bbox):\n",
        "    x1, y1, x2, y2 = bbox_info\n",
        "    x1, y1, x2, y2 = (\n",
        "        (x1 * owidth) - (x2/2) * owidth,\n",
        "        (y1 * oheight) - (y2/2) * oheight,\n",
        "        (x1 * owidth) + (x2/2) * owidth,\n",
        "        (y1 * oheight) + (y2/2) * oheight,\n",
        "        )\n",
        "    print(x1, y1, x2, y2)\n",
        "\n",
        "    # 바운딩 박스 그리기\n",
        "    plt.plot([x1, x2, x2, x1, x1], [y1, y1, y2, y2, y1], color=\"red\")\n",
        "\n",
        "    # 라벨 텍스트 추가\n",
        "    plt.text(x1, y1, f\"Image {i + 1}\", color=\"white\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNet V2"
      ],
      "metadata": {
        "id": "0juITHs9ReLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from typing import Callable, Any, Optional, List\n",
        "\n",
        "\n",
        "__all__ = ['MobileNetV2', 'mobilenet_v2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int, # 입력 특징 맵의 채널 수\n",
        "        out_planes: int, # 출력 특징 맵의 채널 수\n",
        "        kernel_size: int = 3, # 필터 크기\n",
        "        stride: int = 1, # 필터의 이동 거리\n",
        "        groups: int = 1, # 연산에 사용되는 그룹 수\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None # 정규화 레이어 클래스 기본값 nn.BatchNorm2d\n",
        "    ) -> None:\n",
        "        padding = (kernel_size - 1) // 2 # 입력에 추가되는 패딩의 양, 커널 크기가 홀수일 때 중앙 정렬\n",
        "\n",
        "        # 레이어 쌓기\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            # 입력 특징 맵을 출력 특징 맵으로 변환하는 컨볼루션레이어 생성\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "\n",
        "            # 컨볼루션 결과에 적용되는 정규화 레이어 생성 기본값 BatchNorm2d, 출력 채널 수에 맞게 채널별 정규화 진행\n",
        "            norm_layer(out_planes),\n",
        "\n",
        "            # 활성화 함수 ReLU6(0보다 작으면 0, 6보다 크면 6), inplace=True는 입력을 직접 수정\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int, # 입력 채널 수\n",
        "        oup: int, # 출력 채널 수\n",
        "        stride: int, # 필터 이동 거리\n",
        "        expand_ratio: int, # 확장 비율, 입력 채널을 얼마나 확장할지\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None # 정규화 기본 레이어 batchnorm2d\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        # stride는 반드시 1 또는 2이어야 하므로 조건을 걸어 둡니다.\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        # expansion factor를 이용하여 channel을 확장합니다.\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        # stride가 1인 경우에만 residual block을 사용합니다.\n",
        "        # skip connection을 사용하는 경우 input과 output의 크기가 같아야 합니다.\n",
        "        self.use_res_connect = (self.stride == 1) and (inp == oup)\n",
        "\n",
        "        # Inverted Residual 연산\n",
        "        # 레이어 쌓기\n",
        "        layers: List[nn.Module] = []\n",
        "        if expand_ratio != 1:\n",
        "            # point-wise convolution\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
        "\n",
        "        layers.extend([\n",
        "            # depth-wise convolution\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
        "            # point-wise linear convolution\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            norm_layer(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # use_res_connect인 경우만 connection을 연결합니다.\n",
        "        # use_res_connect : stride가 1이고 input과 output의 채널 수가 같은 경우 True\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 1000,\n",
        "        width_mult: float = 1.0,\n",
        "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
        "        round_nearest: int = 8,\n",
        "        block: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "            norm_layer: Module specifying the normalization layer to use\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        # t : expansion factor\n",
        "        # c : output channel의 수\n",
        "        # n : 반복 횟수\n",
        "        # s : stride\n",
        "        if inverted_residual_setting is None:\n",
        "            inverted_residual_setting = [\n",
        "                # t, c, n, s\n",
        "                [1, 16, 1, 1],\n",
        "                [6, 24, 2, 2],\n",
        "                [6, 32, 3, 2],\n",
        "                [6, 64, 4, 2],\n",
        "                [6, 96, 3, 1],\n",
        "                [6, 160, 3, 2],\n",
        "                [6, 320, 1, 1],\n",
        "            ]\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        features: List[nn.Module] = [ConvBNReLU(3, input_channel, stride=2, norm_layer=norm_layer)]\n",
        "\n",
        "\n",
        "        # Inverted Residual Block을 생성합니다.\n",
        "        # features에 feature들의 정보를 차례대로 저장합니다.\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            # width multiplier는 layer의 채널 수를 일정 비율로 줄이는 역할을 합니다.\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
        "                input_channel = output_channel\n",
        "\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
        "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
        "        x = self.features(x)\n",
        "        # Cannot use \"squeeze\" as batch-size can be 1 => must use reshape with x.shape[0]\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1)).reshape(x.shape[0], -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def mobilenet_v2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> MobileNetV2:\n",
        "    \"\"\"\n",
        "    Constructs a MobileNetV2 architecture from\n",
        "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = MobileNetV2(**kwargs)\n",
        "    if pretrained:\n",
        "        try:\n",
        "            from torch.hub import load_state_dict_from_url\n",
        "        except ImportError:\n",
        "            from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "        state_dict = load_state_dict_from_url(\n",
        "            'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth', progress=True)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    net = mobilenet_v2(True)"
      ],
      "metadata": {
        "id": "KsrIbIrC1Gbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다른 버전\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import os\n",
        "\n",
        "# 일반 3x3 convolution\n",
        "def conv_3x3(in_channel, out_channel, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=1),\n",
        "        nn.BatchNorm2d(out_channel),\n",
        "        nn.ReLU6(inplace=True)\n",
        "    )\n",
        "\n",
        "# 일반 1x1 convolution\n",
        "def conv_1x1(in_channel, out_channel):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(out_channel),\n",
        "        nn.ReLU6(inplace=True)\n",
        "    )\n",
        "\n",
        "# pointwise용 1x1 convolution\n",
        "def pointwise_conv(in_channel, out_channel):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(out_channel)\n",
        "    )\n",
        "\n",
        "# depthwise용 3x3 convolution\n",
        "def depthwise_conv(in_channel, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channel, in_channel, kernel_size=3, stride=stride, groups=in_channel, padding=1),\n",
        "        nn.BatchNorm2d(in_channel),\n",
        "        nn.ReLU6(inplace=True)\n",
        "    )\n",
        "\n",
        "# 기본적으로 사용되는 역 Residual Block\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, stride, expand_ratio):\n",
        "        super(InvertedResidual,self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        # expand_ratio = Inverted Residual block 안에서 채널을 얼마나 확장시키는지에 대한 값, 본 논문에서는 t=6으로 하여 확장시키고 있음\n",
        "        # inverted residual 내부 확장된 채널값을 가지게 하기 위함\n",
        "        hidden_dim = int(in_channel * expand_ratio)\n",
        "\n",
        "        # skip connection이 가능한지 조건을 줌\n",
        "        self.use_res_connect = self.stride == 1 and in_channel == out_channel\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # inverted residual 내부에서 확장시켜야 할 때\n",
        "        if expand_ratio != 1:\n",
        "            # 확장시키는 1x1 convolution 사용\n",
        "            layers.append(conv_1x1(in_channel,hidden_dim))\n",
        "        # 확장시키던지 안시키던지 그 후에는 depthwise와 pointwise를 진행하여 함, 이는 MobileNetV1에서 나온 이론에 근거\n",
        "        layers.extend([\n",
        "            # depth wise convolution\n",
        "            depthwise_conv(hidden_dim, stride=stride),\n",
        "            # point wise convolution\n",
        "            pointwise_conv(hidden_dim, out_channel)\n",
        "        ])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## skip connection 조건이 맞으면 해줌\n",
        "        if self.use_res_connect:\n",
        "            return x + self.layers(x)\n",
        "        else :\n",
        "            return self.layers(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self,img_channel, n_class):\n",
        "        super(MobileNetV2,self).__init__()\n",
        "        # 첫 번째 c 값\n",
        "        input_channel = 32\n",
        "        # 마지막 c 값\n",
        "        last_channel = 1280\n",
        "\n",
        "        # 본 논문에서 inverted residual block에 할당하는 값\n",
        "        # t : 확장 비율, 기본으로 설정된 channel 값에 해당 숫자를 곱해서 사용\n",
        "        # c : out_channel 값\n",
        "        # n : 반복 횟수\n",
        "        # s : 첫 번째 반복시 stride\n",
        "        residual_setting = [\n",
        "            #t, c,  n, s\n",
        "            [1, 16, 1, 1],\n",
        "            [6, 24, 2, 2],\n",
        "            [6, 32, 3, 2],\n",
        "            [6, 64, 4, 2],\n",
        "            [6, 96, 3, 1],\n",
        "            [6, 160, 3, 2],\n",
        "            [6, 320, 1, 1]\n",
        "        ]\n",
        "\n",
        "        # 첫 번째 layer\n",
        "        self.first_conv = conv_3x3(img_channel, input_channel, stride = 2)\n",
        "\n",
        "        # bottleneck 구조의 Inverted Residual block layers\n",
        "        layers = []\n",
        "        for t, c, n, s in residual_setting:\n",
        "            # 반복의 첫 번째 s만 지정된 stride값으로 입력 그 외에는 1로 바꿈 그 이유는 Downsampling은 한 번만 해야하므로..\n",
        "            for i in range(n):\n",
        "                if i == 0:\n",
        "                    stride = s\n",
        "                else :\n",
        "                    stride = 1\n",
        "                layers.append(InvertedResidual(input_channel, c, stride = stride, expand_ratio= t))\n",
        "                input_channel = c\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        # 마지막 layer\n",
        "        self.last_conv = conv_1x1(input_channel, last_channel) #input = 320 / output = 1280\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Linear(last_channel,n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first_conv(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.last_conv(x)\n",
        "        x = self.avg_pool(x).view(-1, 1280)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# 모델 확인\n",
        "if __name__==\"__main__\":\n",
        "    model = MobileNetV2(img_channel=3, n_class=1000)\n",
        "    summary(model,(3,224,224), device = \"cpu\")"
      ],
      "metadata": {
        "id": "UX-8uJV67UPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cuda 사용 가능 시 사용\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "# 변수설정\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "data_dir = '/content/bread-1'\n",
        "# /content/bread-1\n",
        "\n",
        "\n",
        "\n",
        "# transform 설정\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = MobileNetV2(img_channel=3, n_class=10).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 비용함수, 옵티마이저 정의\n",
        "\n",
        "# \"Cross-Entropy\"는 두 확률 분포 간의 차이를 측정하는 방법 중 하나\n",
        "# 딥 러닝에서는 일반적으로 모델의 출력을 확률로 해석하고,\n",
        "# 실제 클래스 레이블과 예측 확률 사이의 차이를 최소화하는 것이 목표\n",
        "# 모델 출력과 실제 레이블 간의 차이 계산: 모델의 출력은 클래스에 대한 확률 분포로 해석됩니다.\n",
        "# Cross-Entropy 손실은 모델의 출력과 실제 레이블 간의 차이를 계산합니다.\n",
        "# 손실 값 계산: 계산된 차이를 바탕으로 손실 값을 계산합니다. 이 손실 값은 모델이 얼마나 잘 예측하고 있는지를 나타냅니다.\n",
        "# 역전파 수행: 손실 값을 사용하여 역전파를 수행하여 모델의 가중치를 업데이트합니다.\n",
        "# 이를 통해 모델이 더 나은 예측을 할 수 있도록 학습됩니다.\n",
        "# 매개변수\n",
        "# weight (optional): 각 클래스에 대한 손실 가중치를 지정할 수 있는 매개변수입니다.\n",
        "# 이를 사용하여 특정 클래스에 대한 오류에 더 큰 패널티를 부여하거나 무시할 수 있습니다.\n",
        "# ignore_index (optional): 무시해야 할 클래스 레이블의 인덱스를 지정할 수 있는 매개변수입니다.\n",
        "# 이를 사용하여 손실을 계산할 때 특정 클래스를 무시할 수 있습니다.\n",
        "\n",
        "# 모델의 가중치를 업데이트하는 데 사용되는 옵티마이저\n",
        "# 확률적 경사 하강법(SGD)의 변형 중 하나로, 모멘텀(momentum) 및 이동 평균(rolling average) 기법을 사용하여 학습 속도를 조절하는 방법\n",
        "# Adaptive Learning Rate: 각 파라미터마다 동적으로 학습률을 조정합니다.\n",
        "# 이는 각 파라미터에 대해 학습률을 자동으로 조절하여 최적의 학습 속도를 찾을 수 있도록 합니다.\n",
        "# Momentum: Adam은 모멘텀을 사용하여 이전 그래디언트의 지수 가중 이동 평균을 유지합니다.\n",
        "# 이를 통해 기울기의 방향을 예측하고, 빠른 수렴을 도와줍니다.\n",
        "# Bias-Correction: Adam은 편향 보정(bias-correction)을 수행하여 초기에 그래디언트 추정치가 편향되는 것을 보정합니다.\n",
        "# 매개변수\n",
        "# params: 최적화해야 하는 파라미터들의 리스트입니다. 주로 모델의 parameters() 메서드를 사용하여 이를 전달합니다.\n",
        "# lr (학습률): 학습률은 각 파라미터의 가중치 업데이트에 적용되는 스케일링 요소입니다.\n",
        "# 적절한 학습률은 학습 과정의 안정성과 수렴 속도에 영향을 미칩니다.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "best_acc = 0"
      ],
      "metadata": {
        "id": "B0L9U9I0e7B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch에서 가져오기\n",
        "from torchvision import models\n",
        "mobilenet_v2 = models.mobilenet_v2()\n",
        "# summary(mobilenet_v2, (3,224,224), device=\"cpu\")"
      ],
      "metadata": {
        "id": "My1NfoZoeHJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치 사이즈 (Batch Size): 배치 사이즈는 한 번에 모델에 공급되는 데이터의 샘플\n",
        "  수를 나타냅니다. 예를 들어, 배치 사이즈가 32인 경우, 모델은 32개의 데이터 포인트로 한 번에 학습됩니다.  \n",
        "\n",
        "에포크 (Epoch): 에포크는 전체 데이터셋이 모델에 대해 한 번 학습되는 것을   나타냅니다. 따라서 1 에포크는 전체 데이터셋에 대한 단일 학습 주기를 의미합니다.   에포크가 끝나면 모델은 새로운 에포크에 대한 학습을 시작합니다.  \n",
        "\n",
        "배치 사이즈와 에포크는 모두 모델의 학습 과정에 영향을 미칩니다. 더 큰 배치 사이즈는 메모리 사용량을 늘리지만 학습 속도를 높일 수 있습니다. 반면 더 작은 배치 사이즈는 메모리를 적게 사용하지만 학습 속도가 느릴 수 있습니다. 에포크 수는 모델이 데이터셋을 얼마나 많이 학습하는지를 결정하며, 학습 과정의 안정성과 정확도에 영향을 줄 수 있습니다."
      ],
      "metadata": {
        "id": "oDe4EtuLejBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "기존 cnn 연산량 많고 메모리 사용량 많음.  \n",
        "-> 모바일에 적합하지 않아 모바일에 적합한 모델 구현  \n",
        "\n",
        "MobileNetV1  \n",
        ": Depthwise Separable Convolution (depthwise conv + pointwise conv)  \n",
        ": 세로 방향 채널 3개를 depthwise conv 과 pointwise conv 으로 나누어서 실행  \n",
        ": depthwise conv 은 채널을 하나씩 떼어서 채널 하나에 대해 각각 다른 필터로 컨볼루션 한 다음에 붙이는 작업  \n",
        ": pointwise conv = one by one conv 의 다른 이름  \n",
        "=> 실행하면 스탠다드 3X3 컨볼루션과 똑같이 되나 스탠다드 컨볼루션은 3D 에서 하는데 가로 세로 방향의 2D와 채널방향의 1D로 연산하는데  \n",
        "mobilenet은 2개로 나누어서\n",
        "depthwise로 가로 세로 방향으로만 연산하고  \n",
        "pointwise로 채널 방향만 고려해서 연산  \n",
        "즉 분리시켜 연산량 등을 많이 줄인 모델\n",
        "연산량 비교하면 약 8~9배 정도 줄임  \n",
        "\n",
        "3x3 과 1x1을 합쳐서 하나의 컨볼루션으로 만듦  \n",
        "3X3 depthwise conv  \n",
        "        ↓  \n",
        "        BN  \n",
        "        ↓  \n",
        "      ReLu (exception 논문은 사이에 쓰지 않는게 좋다고 하긴 함)  \n",
        "        ↓  \n",
        "    1X1 conv  \n",
        "        ↓  \n",
        "        BN  \n",
        "        ↓  \n",
        "      ReLu        \n",
        "\n",
        "\n",
        "with multiplier = 채널수를 기본을 1이라 했을 때 3/4, 2/4, 1/4(0.75, 0.5, 0.25) 로 줄이는 것  \n",
        "\n",
        "resolution multiplier = 들어가는 이미지의 레졸루션을 일정한 비율로 줄이는 것  "
      ],
      "metadata": {
        "id": "s5bEJfkrRnw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모바일넷 V2\n",
        "depthwise separable convolutions + Linear Bottlenecks + Inverted Residuals\n",
        "\n",
        "## Linear Bottlenecks\n",
        ": 이미지을 받았을 때 레이어들이 manifold of interest(데이터가 특정한 방식으로 조직되어 있는 형태나 구조를 나타냄) 형성한다고 사람들은 생각함, 저차원 임베딩(: 특정한 데이터를 고차원의 벡터 공간으로 매핑, 이미지를 임베딩하여 고차원의 이미지 데이터를 저차원의 벡터로 표현),\n",
        "1. ReLU 통과 후 선형을 따른다. (음수는 0 양수는 냅두는 형태)\n",
        "=> 로우 디멘젼으로 채널 수 줄이는 거 할 때 Linear bottleneck layer를 쓰면 더 효과적으로 할 수 있음 (ReLU 안 쓰고 ReLU를 쓴게 결국 Linear transform 한거처럼 동작하니), bottleneck 쓸 때 ReLU 쓰면 성능 떨어짐\n",
        "\n",
        "## Residual Blocks\n",
        "처음에 들어가는 것은 채널이 많은 구조에서 one by one 컨볼루션 bottleneck으로 채널 수를 줄인 다음에 필요한 3X3하고 Residual connection이 끝에 와서 다시 더해져야 하니까 이거랑 디멘젼이랑 맞추기 위해 one by one 컨볼루션을 다시 채널을 늘리는 작업을 구성함.\n",
        "wide -> narrow(bottleneck) -> wide approach\n",
        "\n",
        "\n",
        "## Inverted Residuals\n",
        " 이 논문에서는 반대로 한다.\n",
        "one by one 컨볼루션이 relu없이 linear bottleneck이 오퍼레이션 할 때 큰 채널 수에서 작은 거로 갈 때 큰 채널에 있는 정보를 잃지 않고 다 담고 있다고 가정하기때문에 정보를 다 담고 있는 거에서 Residual connection을 만들어도 문제가 없다고 생각하는 것이다. 이렇게 하면 메모리 사용량에 있어서도 효율적이다.\n",
        "정 반대로 처음에 작은 narrow -> wide -> narrow approach 로 된다. 앞에서 쓴거와 정 반대로 작용하기에 inverted residuals라고 이름을 붙인다.\n",
        "\n",
        "# 합치면 Bottelneck Residual Blocks : 기본 빌딩 블록\n",
        ": 내로우 상태로 입력이 들어오면 1X1 expansion layer를 써서 채널 키우고 -> batch nomalization -> ReLU6(활성화 함수, 6이상 되는 애는 6으로)  -> 3X3 depthwise Conv -> batch -> ReLU6 -> 1X1 Projection Layer (=Linear bottleneck: 채널수 다시 줄임) -> batch -> residual connection 만나서 덧셈함\n",
        ": 중간에 채널을 얼만큼 팽창시킬건지 expansion factor t = 6 으로만 진행 (5~10 정도가 성능이 가장 좋았다고 논문에서 말함)\n",
        ": 식만 보면 v1 보다 연산량이 더 많아 보이지만 실제로 보면 채널수가 v2가 더 적은 채널을 가지고 할 수 있어 실제로는 연산량이 60% 정도로 수준이다.\n",
        ": 네트워크 expressiveness 는 expansion layer에서 얼마나 채널 수를 늘려주냐에 따라 결정되고\n",
        "capacity는 projection layer에서 얼마나 줄여주냐에 따라 관련이 있다. 안에서 분리가 되어 있으니 따로 따로 컨트롤 할 수 있으니 좋다라고 설명\n",
        "\n",
        "### tip. stride\n",
        ": 합성곱 신경망(Convolutional Neural Network, CNN)에서 필터(또는 커널)가 입력 데이터를 순회하는 간격을 나타냄. 스트라이드는 필터가 입력 데이터를 얼마나 이동할지를 결정하며, 필터가 한 번에 이동하는 거리를 나타냄\n",
        ": 스트라이드를 사용하면 출력 특성 맵의 크기를 조절할 수 있습니다. 보통 스트라이드가 1인 경우, 필터는 한 칸씩 이동하여 입력 데이터를 순회합니다. 이 경우 출력 특성 맵의 크기는 입력 특성 맵의 크기와 동일합니다. 하지만 스트라이드가 2인 경우, 필터는 두 칸씩 이동하여 입력 데이터를 순회하므로 출력 특성 맵의 크기는 입력 특성 맵의 크기의 반으로 줄어듭니다.\n",
        ":  스트라이드가 커질수록 공간적인 정보의 손실이 발생할 수 있으므로, 적절한 스트라이드 값을 선택하는 것이 중요\n",
        "\n",
        "### tip. resolution = 해상도\n",
        "\n",
        "## Hyper Parameters\n",
        "- input resolution ( 96 ~ 224 )\n",
        "- width multiplier ( 0.35 ~ 1.4 ) - 채널 수 늘렸다 줄였다.\n",
        "\n",
        ": 메모리 효율적으로 쓸 수 있다. bottleneck block 안에서 메모리를 언제 어디서 왔다 갔다 해야하는지 잘 생각하면 처음과 마지막에서 사용하면 내부에서는 내부에서 처리할 수 있다고 생각하니\n",
        "메모리 오퍼레이션이 처음과 마지막에서 일어난다고 했을 때 처음이 작으면 작을수록 좋으니 내로우한 채널을 가지고 있으면 더 유리하다는 얘기.\n",
        ": 여기서 더 메모리를 효율적으로 쓸 수 있게 하는 방법도 설명\n",
        "채널을 n조각 내서 몇개 채널을 먼저 계산해서 보내는 식으로 하면 메모리를 좀 더 효과적으로 쓸 수 있으나 너무 잘게 자르면 성능이 떨어지니 2~5정도로 하면 굉장히 효율적으로 할 수 있다고 말하고 있음\n",
        "\n",
        "# stride = 1 VS stride = 2 비교\n",
        "\n",
        "1일때만 residual connection이 있다.\n",
        "2 일때는 왜 없냐?\n",
        "추측하면 가로 세로가 줄어드니 레지듀얼 커넥션이 날라갈 때 (레지듀얼 커넥션은 마지막에 더해진다.) 이미 expansion하면서 채널 수를 만들었기 때문에 그리고 1X1 layer에서 또 한번 조정할 거기 때문에 그거에 맞게 residual connection도 2배로 늘리는거 하기 싫으니 그냥 안 쓴거 같다.\n",
        "이렇게 해도 성능에 크게 문제 없는거로 추청됨\n",
        "\n",
        "## SSDLite\n",
        ": 바운딩 박스에 대한거를 중간중간 피쳐맵에서 뽑아서 가져가는데 3X3 컨볼루션 연산을 하는데 이 연산을 다 depthwise sepaable 연산으로 바꾼것\n",
        "\n",
        ": 객체 검출에서\n",
        "=> 연산량 굉장히 많이 줄인다. 파람수도 많이 줄임\n",
        "==> MNet V2 + SSDLite 하면 성능이 굉장히 좋다.\n",
        "\n",
        "\n",
        ": 시맨틱 분할(semantic segmentation: 이미지의 각 픽셀을 해당하는 클래스 레이블로 분류하는 작업을 의미, 이미지에서 객체의 경계를 식별하는 것이 아니라, 각 픽셀을 해당하는 클래스에 할당하여 이미지를 구분하는 것)에서\n",
        "=> MNet V2 + deeplabv3 : resnet 보다는 성능이 약간 낮지만 파람수나 연산량이 비교가 안될 정도로 좋다.\n",
        "\n",
        ": 임베딩 시스템에서 구현한거라 모바일 쪽에서 하는 게 좋다.\n",
        "레즈넷은 마이크로 소프트 리서치에서 만든거라 환경이 다르다"
      ],
      "metadata": {
        "id": "a03din3eVJYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNet V1 구현 코드 1"
      ],
      "metadata": {
        "id": "DYRC3nUPEwZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "ElVuj1NF1k9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  2D 컨볼루션, 필터수, 필터크기, 제로패팅, relu 활성화 함수\n",
        "# 배치 정규화 레이어 모델에 추가\n",
        "def add_block(model, num_filters):\n",
        "  model.add(layers.Conv2D(num_filters, (1,1), padding='same', activation='relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.DepthwiseConv2D((3,3), padding='same', activation='relu'))\n",
        "  model.add(layers.BatchNormalization())"
      ],
      "metadata": {
        "id": "Ots2jWi4BYuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA4FEjm2CAHb",
        "outputId": "98a8d0f1-6573-4e7f-a047-49dcf5925f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 크기 변환\n",
        "x_train=x_train.reshape(-1,28,28,1)\n",
        "x_test= x_test.reshape(-1,28,28,1)\n",
        "x_train, x_test = x_train / 255.0, x_test /255.0"
      ],
      "metadata": {
        "id": "jfcal-lcCAfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈 모델 생성\n",
        "# Sequential 모델은 레이어를 선형으로 쌓아 구성하는 가장 간단한 신경망 구조\n",
        "# add() 메서드를 사용하여 각 레이어를 순차적으로 추가한 후,\n",
        "# compile() 메서드를 사용하여 모델을 컴파일하고 학습을 시작할 준비\n",
        "model=models.Sequential()"
      ],
      "metadata": {
        "id": "K8T7a6WXCT69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 784개의 입력을 받는 Dense 레이어 하나와 10개의 출력을 가지는 Dense 레이어 하나로 이루어진 Sequential 모델을 생성\n",
        "# Conv2D 레이어를 추가\n",
        "# 32= 필터(또는 커널)의 수(감지하고자 하는 특징의 수 결정)\n",
        "# (3,3) = 필터의 크기\n",
        "# padding='same': 컨볼루션 연산, 제로 패딩, 입력 이미지 크기 = 출력 이미지 크기\n",
        "# 활성화 함수 relu 사용 (음수 입력에 대해 0으로 출력하고 양수 입력은 그대로 출력하는 비선형 함수)\n",
        "# ㄴ check 필요 relu6 사용한다고 한거 같은데??\n",
        "# input_shape=(28,28,1) : 28X28 크기의 그레이스케일 이미지 입력(1차원)\n",
        "# => 28X28 크기의 그레이스케일 이미지를 32개의 3X3 필터로 컨볼루션 후 relu 활성화 함수 적용하는 Conv2D 레이어 구축\n",
        "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(28,28,1)))"
      ],
      "metadata": {
        "id": "05WpotUQCW6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 정규화 레이어 모델에 추가\n",
        "# 배치 정규화는 신경망의 안정성과 학습 속도를 향상시키기 위해 사용되는 기술 중 하나\n",
        "model.add(layers.BatchNormalization())"
      ],
      "metadata": {
        "id": "vYSIA500HDFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filters에 있는 값들을 순회하면서 모델에 add_block 함수를 사용하여 블록을 추가하는 것\n",
        "# 각 값은 모델에 추가할 컨볼루션 레이어의 필터 수\n",
        "filters=[64,128,256,512,1024]\n",
        "\n",
        "#  주어진 개수의 필터를 갖는 컨볼루션 블록을 모델에 추가하는 함수\n",
        "for num_filters in filters:\n",
        "  add_block(model, num_filters)"
      ],
      "metadata": {
        "id": "vvV-BdvrCndn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten 레이어를 모델에 추가하는 함수 : 다차원 배열을 1차원으로 펼쳐주는 역할, 완전 연결 레이어를 사용할 때 사용\n",
        "# 이미지 처리에서는 일반적으로 Convolutional Neural Network(CNN)을 통해 특징을 추출하고,\n",
        "# 이후에 완전 연결 레이어를 사용하여 분류를 수행\n",
        "# CNN의 출력은 다차원 배열로 나타나는데, 이를 Flatten 레이어를 통해 1차원으로 변환하여 완전 연결 레이어의 입력으로 사용\n",
        "model.add(layers.Flatten())"
      ],
      "metadata": {
        "id": "JvPku2ahCwRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense 레이어는 fully connected 레이어로, 이전 레이어의 모든 뉴런과 연결되어 있는 레이어\n",
        "# 10: 레이어의 출력 뉴런의 수, 출력 클래스의 개수롸 동일해야 함\n",
        "# 10개의 출력을 가진 fully connected 레이어 생성\n",
        "# 10가지의 클래스 중 하나를 예측하는 다중 클래스 분류 작업에 사용\n",
        "\n",
        "# 활성화 함수 softmax : 다중 클래스 분류 문제에서 출력 확률을 계산하는 데 사용\n",
        "# 출력 벡터의 각 요소는 해당 클래스에 대한 확률\n",
        "# 출력을 확률 분포로 변환하여 모든 클래스에 대한 확률의 합이 1이 되도록 함\n",
        "# => 10개의 출력을 가진 fully connected 레이어를 모델에 추가하고\n",
        "#    이 레이어의 출력에 softmax 활성화 함수를 적용하여 다중 클래스 분류를 수행할 준비 완료\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "f6YdgD5MOpYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 10시 7분 진행"
      ],
      "metadata": {
        "id": "vu1UmVqJMowj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 컴파일\n",
        "# => adam 옵티마이저로 모델 컴파일, 손실함수 sparse_categorical_crossentropy 사용, 성능지표로 정확도(Accuracy) 사용\n",
        "\n",
        "# optimizer='adam' : Adam은 경사 하강법의 한 종류로, 모델의 가중치를 업데이트하는데 사용되는 최적화 알고리즘 중 하나\n",
        "#                   Adam은 다양한 하이퍼파라미터를 자동으로 조정하여 최적의 학습률을 찾아줌\n",
        "\n",
        "# loss='sparse_categorical_crossentropy': 손실 함수(loss function) sparse categorical crossentropy\n",
        "# 다중 클래스 분류 문제에서 사용되는 손실 함수\n",
        "# 정수 형태의 클래스 레이블을 입력으로 받아 확률 분포와 비교하여 손실을 계산\n",
        "# 출력 레이어의 활성화 함수로 softmax를 사용하는 경우에 주로 사용\n",
        "\n",
        "# metrics=['accuracy']: 모델의 성능을 평가할 지표(metric)로 정확도(accuracy)를 선택"
      ],
      "metadata": {
        "id": "5Buyi85ZP0RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "7NOYTDTzC5SQ",
        "outputId": "627dbee1-e8c8-4a27-b33a-02852cf2e503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8233s 4s/step - loss: 0.3483 - accuracy: 0.9649 - val_loss: 1.2331 - val_accuracy: 0.9236\n",
            "Epoch 2/10\n",
            " 840/1875 [============>.................] - ETA: 1:10:23 - loss: 0.1971 - accuracy: 0.9794"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a2cbe6e15b58>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "# verbose=0 : 출력을 얼마나 자세하게 표시할지 제어,\n",
        "#             0: 출력을 표시하지 않음을 의미\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)"
      ],
      "metadata": {
        "id": "gLKlrHMUDMQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test Accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "l_qQTyJRDoIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J66EUfZPDtRJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
